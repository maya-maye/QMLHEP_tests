Comment on Quantum Computing and Quantum Machine Learning:

  I'm really excited about leveraging quantum advantage in machine learning to enhance model performance. I have two thoughts on quantum computing and quantum machine learning in particular.

First, nonlinearity plays a crucial role in machine learning, especially in activation functions. Many current QML models still rely on classical activation functions. For example, when working on other tasks I read that Quantum Vision Transformers is using classical GELU. I think incorporating activation functions directly into quantum circuits could improve coherence of the process. In a quantum algorithms course, I learned about Quantum Signal Processing, which encodes polynomial transformations on unitary operations. Since most activation functions can be approximated by polynomials, they could be effectively implemented through QSP. In class, I encoded a sigmoid function using this technique, and I believe hybrid QML models could benefit from a similar approach.

Another key challenge is implementing QML on real quantum hardware. Error correction remains the biggest obstacle to practical quantum computing. In my research project, I have been working on improving probabilistic error cancellation on various circuits. While QML models often require fewer qubits and gates compared to classical counterparts, training large models on complex data still demands deep circuits and many qubits. However, I became optimistic about this challenge when learning about the analogy of Quantum circuits and the human brain. In the brain, individual neurons operate with a lot of noise, yet when connected together, they realize precise cognition. Current quantum computers are similarly noisy, suggesting a strong similarity. While we havenâ€™t yet found a way to harness this noise collectively to achieve perfect observables, perhaps one day, we will find the method of realizing perfect QML on Quantum Computer through learning more about human brain.
